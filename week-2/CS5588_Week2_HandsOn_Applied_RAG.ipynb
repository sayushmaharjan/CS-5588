{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8beb036f",
   "metadata": {},
   "source": [
    "# CS 5588 — Week 2 Hands-On: Applied RAG for Product & Venture Development (Two-Step)\n",
    "**Initiation (20 min, Jan 27)** → **Completion (60 min, Jan 29)**\n",
    "\n",
    "**Submission:** Survey + GitHub  \n",
    "**Due:** **Jan 29 (Thu), end of class**\n",
    "\n",
    "## New Requirement (Important)\n",
    "For **full credit (2% individual)** you must:\n",
    "1) Use **your own project-aligned dataset** (not only benchmark)  \n",
    "2) Add **your own explanations** for key steps\n",
    "\n",
    "### ✅ “Cell Description” rule (same style as CS 5542)\n",
    "After each **IMPORTANT** code cell, add a short Markdown **Cell Description** (2–5 sentences):\n",
    "- What the cell does\n",
    "- Why it matters for a **product-grade** RAG system\n",
    "- Any design choices (chunk size, α, reranker, etc.)\n",
    "\n",
    "> Treat these descriptions as **mini system documentation** (engineering + product thinking).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e43e2d",
   "metadata": {},
   "source": [
    "## Project Dataset Guide (Required for Full Credit)\n",
    "\n",
    "### Minimum requirements\n",
    "- **5–25 documents** (start small; scale later)\n",
    "- Prefer **plain text** documents (`.txt`)\n",
    "- Put files in a folder named: `project_data/`\n",
    "\n",
    "### Recommended dataset types (choose one)\n",
    "- Policies / guidelines / compliance docs\n",
    "- Technical docs / manuals / SOPs\n",
    "- Customer support FAQs / tickets (de-identified)\n",
    "- Research notes / literature summaries\n",
    "- Domain corpus (healthcare, cybersecurity, business, etc.)\n",
    "\n",
    "> Benchmarks are optional, but **cannot** earn full credit by themselves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f68d33",
   "metadata": {},
   "source": [
    "## 0) One-Click Setup + Import Check  ✅ **IMPORTANT: Add Cell Description after running**\n",
    "If you are in **Google Colab**, run the install cell below, then **Runtime → Restart session** if imports fail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaa1c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS 5588 Lab 2 — One-click dependency install (Colab)\n",
    "!pip -q install -U sentence-transformers chromadb faiss-cpu scikit-learn rank-bm25 transformers accelerate\n",
    "\n",
    "import sys, platform\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"✅ If imports fail later: Runtime → Restart session and run again.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab532915",
   "metadata": {},
   "source": [
    "### ✍️ Cell Description (Student)\n",
    "Write 2–5 sentences explaining what the setup cell does and why restarting the runtime sometimes matters after pip installs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49154e13",
   "metadata": {},
   "source": [
    "# STEP 1 — INITIATION (Jan 27, 20 minutes)\n",
    "**Goal:** Define the **product**, **users**, **dataset reality**, and **trust risks**.\n",
    "\n",
    "> This is a **product milestone**, not a coding demo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58216603",
   "metadata": {},
   "source": [
    "## 1A) Product Framing (Required)  ✅ **IMPORTANT: Add Cell Description after running**\n",
    "Fill in the template below like a founder/product lead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214ee1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = {\n",
    "  \"product_name\": \"\",\n",
    "  \"target_users\": \"\",\n",
    "  \"core_problem\": \"\",\n",
    "  \"why_rag_not_chatbot\": \"\",\n",
    "  \"failure_harms_who_and_how\": \"\",\n",
    "}\n",
    "product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490a084a",
   "metadata": {},
   "source": [
    "### ✍️ Cell Description (Student)\n",
    "Explain your product in 3–5 sentences: who the user is, what pain point exists today, and why grounded RAG helps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179e8e12",
   "metadata": {},
   "source": [
    "## 1B) Dataset Reality Plan (Required)  ✅ **IMPORTANT: Add Cell Description after running**\n",
    "Describe where your data comes from **in the real world**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282cb6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_plan = {\n",
    "  \"data_owner\": \"\",              # company / agency / public / internal team\n",
    "  \"data_sensitivity\": \"\",        # public / internal / regulated / confidential\n",
    "  \"document_types\": \"\",          # policies, manuals, reports, research, etc.\n",
    "  \"expected_scale_in_production\": \"\",  # e.g., 200 docs, 10k docs, etc.\n",
    "  \"data_reality_check_paragraph\": \"\",\n",
    "}\n",
    "dataset_plan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2da001",
   "metadata": {},
   "source": [
    "### ✍️ Cell Description (Student)\n",
    "Write 2–5 sentences describing where this data would come from in a real deployment and any privacy/regulatory constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df3ac72",
   "metadata": {},
   "source": [
    "## 1C) User Stories + Mini Rubric (Required)  ✅ **IMPORTANT: Add Cell Description after running**\n",
    "Define **3 user stories** (U1 normal, U2 high-stakes, U3 ambiguous/failure) + rubric for evidence and correctness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a72b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stories = {\n",
    "  \"U1_normal\": {\n",
    "    \"user_story\": \"As a ___, I want to ___ so that I can ___.\",\n",
    "    \"acceptable_evidence\": [\"\", \"\"],\n",
    "    \"correct_answer_must_include\": [\"\", \"\"],\n",
    "  },\n",
    "  \"U2_high_stakes\": {\n",
    "    \"user_story\": \"As a ___, I want to ___ so that I can ___.\",\n",
    "    \"acceptable_evidence\": [\"\", \"\"],\n",
    "    \"correct_answer_must_include\": [\"\", \"\"],\n",
    "  },\n",
    "  \"U3_ambiguous_failure\": {\n",
    "    \"user_story\": \"As a ___, I want to ___ so that I can ___.\",\n",
    "    \"acceptable_evidence\": [\"\", \"\"],\n",
    "    \"correct_answer_must_include\": [\"\", \"\"],\n",
    "  },\n",
    "}\n",
    "user_stories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5189f5",
   "metadata": {},
   "source": [
    "### ✍️ Cell Description (Student)\n",
    "Explain why U2 is “high-stakes” and what the system must do to avoid harm (abstain, cite evidence, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9c075c",
   "metadata": {},
   "source": [
    "## 1D) Trust & Risk Table (Required)\n",
    "Fill at least **3 rows**. These risks should match your product and user stories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972f5b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_table = [\n",
    "  {\"risk\": \"Hallucination\", \"example_failure\": \"\", \"real_world_consequence\": \"\", \"safeguard_idea\": \"Force citations + abstain\"},\n",
    "  {\"risk\": \"Omission\", \"example_failure\": \"\", \"real_world_consequence\": \"\", \"safeguard_idea\": \"Recall tuning + hybrid retrieval\"},\n",
    "  {\"risk\": \"Bias/Misleading\", \"example_failure\": \"\", \"real_world_consequence\": \"\", \"safeguard_idea\": \"Reranking rules + human review\"},\n",
    "]\n",
    "risk_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fe422b",
   "metadata": {},
   "source": [
    "✅ **Step 1 Checkpoint (End of Jan 27)**\n",
    "Commit (or submit) your filled templates:\n",
    "- `product`, `dataset_plan`, `user_stories`, `risk_table`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9645a53",
   "metadata": {},
   "source": [
    "# STEP 2 — COMPLETION (Jan 29, 60 minutes)\n",
    "**Goal:** Build a working **product-grade** RAG pipeline:\n",
    "Chunking → Keyword + Vector Retrieval → Hybrid α → Governance Rerank → Grounded Answer → Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849ea98a",
   "metadata": {},
   "source": [
    "## 2A) Project Dataset Setup (Required for Full Credit)  ✅ **IMPORTANT: Add Cell Description after running**\n",
    "\n",
    "### Colab Upload Tips\n",
    "- Left sidebar → **Files** → Upload `.txt`\n",
    "- Place them into `project_data/`\n",
    "\n",
    "This cell creates the folder and shows how many files were found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a38f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_FOLDER = \"project_data\"\n",
    "os.makedirs(PROJECT_FOLDER, exist_ok=True)\n",
    "\n",
    "# (Optional helper) Move any .txt in current directory into project_data/\n",
    "moved = 0\n",
    "for fp in glob.glob(\"*.txt\"):\n",
    "    shutil.move(fp, os.path.join(PROJECT_FOLDER, os.path.basename(fp)))\n",
    "    moved += 1\n",
    "\n",
    "files = sorted(glob.glob(os.path.join(PROJECT_FOLDER, \"*.txt\")))\n",
    "print(\"✅ project_data/ ready | moved:\", moved, \"| files:\", len(files))\n",
    "print(\"Example files:\", files[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec380ad4",
   "metadata": {},
   "source": [
    "### ✍️ Cell Description (Student)\n",
    "List what dataset you used, how many docs, and why they reflect your product scenario (not just a toy example).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a487a1c7",
   "metadata": {},
   "source": [
    "## 2B) Load Documents + Build Chunks  ✅ **IMPORTANT: Add Cell Description after running**\n",
    "This milestone cell loads `.txt` documents and produces chunks using either **fixed** or **semantic** chunking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a081d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def load_project_docs(folder=\"project_data\", max_docs=25):\n",
    "    paths = sorted(Path(folder).glob(\"*.txt\"))[:max_docs]\n",
    "    docs = []\n",
    "    for p in paths:\n",
    "        txt = p.read_text(encoding=\"utf-8\", errors=\"ignore\").strip()\n",
    "        if txt:\n",
    "            docs.append({\"doc_id\": p.name, \"text\": txt})\n",
    "    return docs\n",
    "\n",
    "def fixed_chunk(text, chunk_size=900, overlap=150):\n",
    "    # Character-based chunking for speed + simplicity\n",
    "    chunks, i = [], 0\n",
    "    while i < len(text):\n",
    "        chunks.append(text[i:i+chunk_size])\n",
    "        i += (chunk_size - overlap)\n",
    "    return [c.strip() for c in chunks if c.strip()]\n",
    "\n",
    "def semantic_chunk(text, max_chars=1000):\n",
    "    # Paragraph-based packing\n",
    "    paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
    "    chunks, cur = [], \"\"\n",
    "    for p in paras:\n",
    "        if len(cur) + len(p) + 2 <= max_chars:\n",
    "            cur = (cur + \"\\n\\n\" + p).strip()\n",
    "        else:\n",
    "            if cur: chunks.append(cur)\n",
    "            cur = p\n",
    "    if cur: chunks.append(cur)\n",
    "    return chunks\n",
    "\n",
    "# ---- Choose chunking policy ----\n",
    "CHUNKING = \"semantic\"   # \"fixed\" or \"semantic\"\n",
    "FIXED_SIZE = 900\n",
    "FIXED_OVERLAP = 150\n",
    "SEM_MAX = 1000\n",
    "\n",
    "docs = load_project_docs(PROJECT_FOLDER, max_docs=25)\n",
    "print(\"Loaded docs:\", len(docs))\n",
    "\n",
    "all_chunks = []\n",
    "for d in docs:\n",
    "    chunks = fixed_chunk(d[\"text\"], FIXED_SIZE, FIXED_OVERLAP) if CHUNKING == \"fixed\" else semantic_chunk(d[\"text\"], SEM_MAX)\n",
    "    for j, c in enumerate(chunks):\n",
    "        all_chunks.append({\"chunk_id\": f'{d[\"doc_id\"]}::c{j}', \"doc_id\": d[\"doc_id\"], \"text\": c})\n",
    "\n",
    "print(\"Chunking:\", CHUNKING, \"| total chunks:\", len(all_chunks))\n",
    "print(\"Sample chunk id:\", all_chunks[0][\"chunk_id\"] if all_chunks else \"NO CHUNKS (upload .txt files first)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204e5e83",
   "metadata": {},
   "source": [
    "### ✍️ Cell Description (Student)\n",
    "Explain why you chose fixed vs semantic chunking for your product, and how chunking affects precision/recall and trust.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bec9a30",
   "metadata": {},
   "source": [
    "## 2C) Build Retrieval Engines (BM25 + Vector Index)  ✅ **IMPORTANT: Add Cell Description after running**\n",
    "This cell builds:\n",
    "- **Keyword retrieval** (BM25) for exact matches / compliance\n",
    "- **Vector retrieval** (embeddings + FAISS) for semantic matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0484f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# ----- Keyword (BM25) -----\n",
    "tokenized = [c[\"text\"].lower().split() for c in all_chunks]\n",
    "bm25 = BM25Okapi(tokenized) if len(tokenized) else None\n",
    "\n",
    "def keyword_search(query, k=10):\n",
    "    if bm25 is None:\n",
    "        return []\n",
    "    scores = bm25.get_scores(query.lower().split())\n",
    "    idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
    "    return [(all_chunks[i], float(scores[i])) for i in idx]\n",
    "\n",
    "# ----- Vector (Embeddings + FAISS) -----\n",
    "EMB_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = SentenceTransformer(EMB_MODEL_NAME)\n",
    "\n",
    "chunk_texts = [c[\"text\"] for c in all_chunks]\n",
    "if len(chunk_texts) > 0:\n",
    "    emb = embedder.encode(chunk_texts, show_progress_bar=True, normalize_embeddings=True)\n",
    "    emb = np.asarray(emb, dtype=\"float32\")\n",
    "\n",
    "    index = faiss.IndexFlatIP(emb.shape[1])\n",
    "    index.add(emb)\n",
    "\n",
    "    def vector_search(query, k=10):\n",
    "        q = embedder.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
    "        scores, idx = index.search(q, k)\n",
    "        out = [(all_chunks[int(i)], float(s)) for s, i in zip(scores[0], idx[0])]\n",
    "        return out\n",
    "    print(\"✅ Vector index built | chunks:\", len(all_chunks), \"| dim:\", emb.shape[1])\n",
    "else:\n",
    "    index = None\n",
    "    def vector_search(query, k=10): return []\n",
    "    print(\"⚠️ No chunks found. Upload .txt files to project_data/ and rerun.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cb1a14",
   "metadata": {},
   "source": [
    "### ✍️ Cell Description (Student)\n",
    "Explain why your product needs both keyword and vector retrieval (what each catches that the other misses).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7dfd29",
   "metadata": {},
   "source": [
    "## 2D) Hybrid Retrieval (α Fusion Policy)  ✅ **IMPORTANT: Add Cell Description after running**\n",
    "Hybrid score = **α · keyword + (1 − α) · vector** after simple normalization.\n",
    "\n",
    "Try α ∈ {0.2, 0.5, 0.8} and justify your choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909589ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_norm(pairs):\n",
    "    scores = np.array([s for _, s in pairs], dtype=\"float32\") if pairs else np.array([], dtype=\"float32\")\n",
    "    if len(scores) == 0:\n",
    "        return []\n",
    "    mn, mx = float(scores.min()), float(scores.max())\n",
    "    if mx - mn < 1e-8:\n",
    "        return [(c, 1.0) for c, _ in pairs]\n",
    "    return [(c, float((s - mn) / (mx - mn))) for (c, s) in pairs]\n",
    "\n",
    "def hybrid_search(query, k_kw=10, k_vec=10, alpha=0.5, k_out=10):\n",
    "    kw = keyword_search(query, k_kw)\n",
    "    vc = vector_search(query, k_vec)\n",
    "    kw_n = dict((c[\"chunk_id\"], s) for c, s in minmax_norm(kw))\n",
    "    vc_n = dict((c[\"chunk_id\"], s) for c, s in minmax_norm(vc))\n",
    "\n",
    "    ids = set(kw_n) | set(vc_n)\n",
    "    fused = []\n",
    "    for cid in ids:\n",
    "        s = alpha * kw_n.get(cid, 0.0) + (1 - alpha) * vc_n.get(cid, 0.0)\n",
    "        chunk = next(c for c in all_chunks if c[\"chunk_id\"] == cid)\n",
    "        fused.append((chunk, float(s)))\n",
    "\n",
    "    fused.sort(key=lambda x: x[1], reverse=True)\n",
    "    return fused[:k_out]\n",
    "\n",
    "ALPHA = 0.5  # try 0.2 / 0.5 / 0.8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4b3559",
   "metadata": {},
   "source": [
    "### ✍️ Cell Description (Student)\n",
    "Describe your user type (precision-first vs discovery-first) and why your α choice fits that user and risk profile.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f888bf",
   "metadata": {},
   "source": [
    "## 2E) Governance Layer (Re-ranking)  ✅ **IMPORTANT: Add Cell Description after running**\n",
    "Re-ranking is treated as **governance** (risk reduction), not just performance tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e2fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "RERANK = True\n",
    "RERANK_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "reranker = CrossEncoder(RERANK_MODEL) if RERANK else None\n",
    "\n",
    "def rerank(query, candidates):\n",
    "    if reranker is None or len(candidates) == 0:\n",
    "        return candidates\n",
    "    pairs = [(query, c[\"text\"]) for c, _ in candidates]\n",
    "    scores = reranker.predict(pairs)\n",
    "    out = [(c, float(s)) for (c, _), s in zip(candidates, scores)]\n",
    "    out.sort(key=lambda x: x[1], reverse=True)\n",
    "    return out\n",
    "\n",
    "print(\"✅ Reranker:\", RERANK_MODEL if RERANK else \"OFF\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bb530f",
   "metadata": {},
   "source": [
    "### ✍️ Cell Description (Student)\n",
    "Explain what “governance” means for your product and what failure this reranking step helps prevent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81bbbd3",
   "metadata": {},
   "source": [
    "## 2F) Grounded Answer + Citations  ✅ **IMPORTANT: Add Cell Description after running**\n",
    "We include a lightweight generation option, plus a fallback mode.\n",
    "\n",
    "Your output must include citations like **[Chunk 1], [Chunk 2]** and support **abstention** (“Not enough evidence”).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ae6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "USE_LLM = False  # set True to generate; keep False if downloads are slow\n",
    "GEN_MODEL = \"google/flan-t5-base\"\n",
    "\n",
    "gen = pipeline(\"text2text-generation\", model=GEN_MODEL) if USE_LLM else None\n",
    "\n",
    "def build_context(top_chunks, max_chars=2500):\n",
    "    ctx = \"\"\n",
    "    for i, (c, _) in enumerate(top_chunks, start=1):\n",
    "        block = f\"[Chunk {i}] {c['text'].strip()}\\n\"\n",
    "        if len(ctx) + len(block) > max_chars:\n",
    "            break\n",
    "        ctx += block + \"\\n\"\n",
    "    return ctx.strip()\n",
    "\n",
    "def rag_answer(query, top_chunks):\n",
    "    ctx = build_context(top_chunks)\n",
    "    if USE_LLM and gen is not None:\n",
    "        prompt = (\n",
    "            \"Answer the question using ONLY the evidence below. \"\n",
    "            \"If there is not enough evidence, say 'Not enough evidence.' \"\n",
    "            \"Include citations like [Chunk 1], [Chunk 2].\\n\\n\"\n",
    "            f\"Question: {query}\\n\\nEvidence:\\n{ctx}\\n\\nAnswer:\"\n",
    "        )\n",
    "        out = gen(prompt, max_new_tokens=180)[0][\"generated_text\"]\n",
    "        return out, ctx\n",
    "    else:\n",
    "        # fallback: evidence-first placeholder\n",
    "        answer = (\n",
    "            \"Evidence summary (fallback mode):\\n\"\n",
    "            + \"\\n\".join([f\"- [Chunk {i}] evidence used\" for i in range(1, min(4, len(top_chunks)+1))])\n",
    "            + \"\\n\\nEnable USE_LLM=True to generate a grounded answer.\"\n",
    "        )\n",
    "        return answer, ctx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c50ed74",
   "metadata": {},
   "source": [
    "### ✍️ Cell Description (Student)\n",
    "Explain how citations and abstention improve trust in your product, especially for U2 (high-stakes) and U3 (ambiguous).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78586432",
   "metadata": {},
   "source": [
    "## 2G) Run the Pipeline on Your 3 User Stories  ✅ **IMPORTANT: Add Cell Description after running**\n",
    "This cell turns your user stories into concrete queries, runs hybrid+rerank, and prints results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606aaafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def story_to_query(story_text):\n",
    "    m = re.search(r\"I want to (.+?)(?: so that|\\.|$)\", story_text, flags=re.IGNORECASE)\n",
    "    return m.group(1).strip() if m else story_text.strip()\n",
    "\n",
    "queries = [\n",
    "    (\"U1_normal\", story_to_query(user_stories[\"U1_normal\"][\"user_story\"])),\n",
    "    (\"U2_high_stakes\", story_to_query(user_stories[\"U2_high_stakes\"][\"user_story\"])),\n",
    "    (\"U3_ambiguous_failure\", story_to_query(user_stories[\"U3_ambiguous_failure\"][\"user_story\"])),\n",
    "]\n",
    "\n",
    "def run_pipeline(query, alpha=ALPHA, k=10, do_rerank=RERANK):\n",
    "    base = hybrid_search(query, alpha=alpha, k_out=k)\n",
    "    ranked = rerank(query, base) if do_rerank else base\n",
    "    top5 = ranked[:5]\n",
    "    ans, ctx = rag_answer(query, top5[:3])\n",
    "    return top5, ans, ctx\n",
    "\n",
    "results = {}\n",
    "for key, q in queries:\n",
    "    top5, ans, ctx = run_pipeline(q)\n",
    "    results[key] = {\"query\": q, \"top5\": top5, \"answer\": ans, \"context\": ctx}\n",
    "\n",
    "for key in results:\n",
    "    print(\"\\n===\", key, \"===\")\n",
    "    print(\"Query:\", results[key][\"query\"])\n",
    "    print(\"Top chunk ids:\", [c[\"chunk_id\"] for c, _ in results[key][\"top5\"][:3]])\n",
    "    print(\"Answer preview:\\n\", results[key][\"answer\"][:500], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ae35f7",
   "metadata": {},
   "source": [
    "### ✍️ Cell Description (Student)\n",
    "Describe one place where the system helped (better grounding) and one place where it struggled (which layer and why).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62b369e",
   "metadata": {},
   "source": [
    "## 2H) Evaluation (Technical + Product)  ✅ **IMPORTANT: Add Cell Description after running**\n",
    "Use your rubric to label relevance and compute Precision@5 / Recall@10.\n",
    "Also assign product scores: Trust (1–5) and Decision Confidence (1–5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7a7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(relevant_flags, k=5):\n",
    "    rel = relevant_flags[:k]\n",
    "    return sum(rel) / max(1, len(rel))\n",
    "\n",
    "def recall_at_k(relevant_flags, total_relevant, k=10):\n",
    "    rel_found = sum(relevant_flags[:k])\n",
    "    return rel_found / max(1, total_relevant)\n",
    "\n",
    "evaluation = {}\n",
    "for key in results:\n",
    "    print(\"\\n---\", key, \"---\")\n",
    "    print(\"Query:\", results[key][\"query\"])\n",
    "    print(\"Top-5 chunks:\")\n",
    "    for i, (c, s) in enumerate(results[key][\"top5\"], start=1):\n",
    "        print(i, c[\"chunk_id\"], \"| score:\", round(s, 3))\n",
    "\n",
    "    evaluation[key] = {\n",
    "        \"relevant_flags_top10\": [0]*10,             # set 1 for each relevant chunk among top-10\n",
    "        \"total_relevant_chunks_estimate\": 0,        # estimate from your rubric\n",
    "        \"precision_at_5\": None,\n",
    "        \"recall_at_10\": None,\n",
    "        \"trust_score_1to5\": 0,\n",
    "        \"confidence_score_1to5\": 0,\n",
    "    }\n",
    "\n",
    "evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f1991f",
   "metadata": {},
   "source": [
    "### ✍️ Cell Description (Student)\n",
    "Explain how you labeled “relevance” using your rubric and what “trust” means for your target users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10840c20",
   "metadata": {},
   "source": [
    "## 2I) Failure Case + Venture Fix (Required)\n",
    "Document one real failure and propose a **system-level** fix (data/chunking/α/rerank/human review).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_case = {\n",
    "  \"which_user_story\": \"\",\n",
    "  \"what_failed\": \"\",\n",
    "  \"which_layer_failed\": \"\",  # Chunking / Retrieval / Re-ranking / Generation\n",
    "  \"real_world_consequence\": \"\",\n",
    "  \"proposed_system_fix\": \"\",\n",
    "}\n",
    "failure_case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437fa43c",
   "metadata": {},
   "source": [
    "## 2J) README Template (Copy into GitHub README.md)\n",
    "\n",
    "```md\n",
    "# Week 2 Hands-On — Applied RAG Product Results (CS 5588)\n",
    "\n",
    "## Product Overview\n",
    "- Product name:\n",
    "- Target users:\n",
    "- Core problem:\n",
    "- Why RAG:\n",
    "\n",
    "## Dataset Reality\n",
    "- Source / owner:\n",
    "- Sensitivity:\n",
    "- Document types:\n",
    "- Expected scale in production:\n",
    "\n",
    "## User Stories + Rubric\n",
    "- U1:\n",
    "- U2:\n",
    "- U3:\n",
    "(Rubric: acceptable evidence + correct answer criteria)\n",
    "\n",
    "## System Architecture\n",
    "- Chunking:\n",
    "- Keyword retrieval:\n",
    "- Vector retrieval:\n",
    "- Hybrid α:\n",
    "- Reranking governance:\n",
    "- LLM / generation option:\n",
    "\n",
    "## Results\n",
    "| User Story | Method | Precision@5 | Recall@10 | Trust (1–5) | Confidence (1–5) |\n",
    "|---|---|---:|---:|---:|---:|\n",
    "\n",
    "## Failure + Fix\n",
    "- Failure:\n",
    "- Layer:\n",
    "- Consequence:\n",
    "- Safeguard / next fix:\n",
    "\n",
    "## Evidence of Grounding\n",
    "Paste one RAG answer with citations: [Chunk 1], [Chunk 2]\n",
    "```\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
