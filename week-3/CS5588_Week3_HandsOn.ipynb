{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a2ae7b1",
   "metadata": {},
   "source": [
    "# CS 5588 — Week 3 Hands-On  \n",
    "## Building a Multimodal RAG Product Prototype (PDF + Images)\n",
    "\n",
    "**Goal (today):** Build a *working product prototype* that answers user questions from real documents (PDFs + images) with **evidence citations**.\n",
    "\n",
    "**What you’ll leave with:**\n",
    "- A project-ready multimodal RAG pipeline (ingestion → indexing → retrieval → grounded answer)\n",
    "- A short **Product Brief** inside the notebook (persona, problem, value, success metrics)\n",
    "- A small **demo loop** you can show to stakeholders (prompt → answer + citations)\n",
    "\n",
    "> This hands-on is application-first: prioritize a realistic use case and a clean demo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25710df",
   "metadata": {},
   "source": [
    "## 0) Product Brief (Fill in — REQUIRED for Week 3)\n",
    "- **Team / Name:**  \n",
    "- **Project name (working title):**  \n",
    "\n",
    "### 0.1 Target user persona\n",
    "- Who will use this? (role, context, pain point)\n",
    "\n",
    "### 0.2 Problem statement (1–2 sentences)\n",
    "- What decision/task does your product support?\n",
    "\n",
    "### 0.3 Value proposition (1 sentence)\n",
    "- What improves (speed, accuracy, trust, cost, risk)?\n",
    "\n",
    "### 0.4 Success metrics (pick 2–3)\n",
    "- e.g., time-to-answer, citation coverage, % “not enough evidence” when missing, user satisfaction (1–5), precision@5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf340d90",
   "metadata": {},
   "source": [
    "## 1) Setup (Colab)\n",
    "Run installs, then imports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfdf4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup & Imports (Colab-friendly) ===\n",
    "import os, re, glob, json, math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Core deps ----\n",
    "# PyMuPDF for PDF text extraction\n",
    "!pip -q install pymupdf pillow pandas numpy scikit-learn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "\n",
    "# ---- OCR deps ----\n",
    "!pip -q install pytesseract\n",
    "!sudo apt-get -qq update\n",
    "!sudo apt-get -qq install -y tesseract-ocr\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "# ---- Retrieval deps ----\n",
    "!pip -q install faiss-cpu rank-bm25\n",
    "import faiss\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# ---- Dense + rerank (optional) ----\n",
    "# Some environments may have version conflicts. We try to install, but fall back gracefully if needed.\n",
    "USE_ST = True\n",
    "USE_RERANK = True\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "except Exception as e:\n",
    "    USE_ST = False\n",
    "    USE_RERANK = False\n",
    "    print(\"⚠️ sentence-transformers not available in this runtime. Falling back to TF-IDF for 'dense' retrieval.\")\n",
    "    print(\"   Error:\", e)\n",
    "\n",
    "# Optional captioning (bonus)\n",
    "USE_CAPTIONING = False\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    USE_CAPTIONING = True\n",
    "except Exception:\n",
    "    USE_CAPTIONING = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c67007",
   "metadata": {},
   "source": [
    "### 1.1 System dependencies (Colab/Linux)\n",
    "If OCR fails, run this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f30307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Handled in Setup & Imports above)\n",
    "print('System dependencies installed in Section 1.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9867888d",
   "metadata": {},
   "source": [
    "### 1.2 Imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "81a8ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "> **Note:** Dependencies are installed and imported above. If you restart the runtime, re-run Sections 1–2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93606e0e",
   "metadata": {},
   "source": [
    "## 2) Choose a project dataset (realistic, stakeholder-facing)\n",
    "Create this structure (you can start small today):\n",
    "\n",
    "```\n",
    "project_data_mm/\n",
    "  docs/\n",
    "    doc1.pdf\n",
    "    doc2.pdf\n",
    "  figures/\n",
    "    fig1.png\n",
    "    fig2.jpg\n",
    "  notes.txt (optional)\n",
    "```\n",
    "\n",
    "**Recommended today:** 2 PDFs + 3–5 images that matter to your use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1668fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"project_data_mm\"\n",
    "DOC_DIR = os.path.join(DATA_DIR, \"docs\")\n",
    "FIG_DIR = os.path.join(DATA_DIR, \"figures\")\n",
    "\n",
    "for d in [DATA_DIR, DOC_DIR, FIG_DIR]:\n",
    "    if not os.path.isdir(d):\n",
    "        print(f\"Missing folder: {d} (create it and add files)\")\n",
    "\n",
    "pdfs = sorted(glob.glob(os.path.join(DOC_DIR, \"*.pdf\")))\n",
    "imgs = sorted(glob.glob(os.path.join(FIG_DIR, \"*.*\")))\n",
    "\n",
    "print(\"PDFs:\", len(pdfs), pdfs[:5])\n",
    "print(\"Images:\", len(imgs), imgs[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cba544b",
   "metadata": {},
   "source": [
    "## 3) Define 3 stakeholder questions (application-oriented)\n",
    "- **Q1/Q2:** require both text + figure/table evidence  \n",
    "- **Q3:** ambiguous/missing evidence → system should say **Not enough evidence in the retrieved context.**\n",
    "\n",
    "Also add:\n",
    "- Must-cite evidence (page or figure)\n",
    "- Success criteria (what a good answer must include)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d49247",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERIES = [\n",
    "    {\"id\":\"Q1\",\"question\":\"TODO\",\"must_cite\":[\"TODO\"],\"success_criteria\":[\"TODO\"],\"keywords\":[\"TODO\"]},\n",
    "    {\"id\":\"Q2\",\"question\":\"TODO\",\"must_cite\":[\"TODO\"],\"success_criteria\":[\"TODO\"],\"keywords\":[\"TODO\"]},\n",
    "    {\"id\":\"Q3\",\"question\":\"TODO\",\"must_cite\":[],\"success_criteria\":[\"Not enough evidence in the retrieved context.\"],\"keywords\":[\"TODO\"]},\n",
    "]\n",
    "for q in QUERIES: \n",
    "    print(q[\"id\"], q[\"question\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2299c",
   "metadata": {},
   "source": [
    "## 4) Ingest PDFs (per-page text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42af3204",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TextChunk:\n",
    "    chunk_id: str\n",
    "    doc_id: str\n",
    "    page_num: int\n",
    "    text: str\n",
    "\n",
    "def extract_pdf_pages(pdf_path: str) -> List[TextChunk]:\n",
    "    doc_id = os.path.basename(pdf_path)\n",
    "    doc = fitz.open(pdf_path)\n",
    "    out = []\n",
    "    for i in range(len(doc)):\n",
    "        page = doc.load_page(i)\n",
    "        text = page.get_text(\"text\") or \"\"\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        if text:\n",
    "            out.append(TextChunk(f\"{doc_id}::p{i+1}\", doc_id, i+1, text))\n",
    "    return out\n",
    "\n",
    "page_chunks = []\n",
    "for p in pdfs:\n",
    "    page_chunks.extend(extract_pdf_pages(p))\n",
    "\n",
    "print(\"Total PDF page chunks:\", len(page_chunks))\n",
    "if page_chunks:\n",
    "    print(\"Sample:\", page_chunks[0].chunk_id, page_chunks[0].text[:250])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7cca6a",
   "metadata": {},
   "source": [
    "## 5) Ingest images (OCR first, optional captioning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe2ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvidenceItem:\n",
    "    evid_id: str\n",
    "    source: str\n",
    "    image_path: str\n",
    "    ocr_text: str\n",
    "    caption_text: str\n",
    "    evidence_text: str\n",
    "\n",
    "def run_ocr(image_path: str) -> str:\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    text = pytesseract.image_to_string(img)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "evidence_items = []\n",
    "for ip in imgs:\n",
    "    base = os.path.basename(ip)\n",
    "    evid_id = os.path.splitext(base)[0]\n",
    "    ocr = run_ocr(ip)\n",
    "    evidence_items.append(EvidenceItem(evid_id, base, ip, ocr, \"\", ocr))\n",
    "\n",
    "print(\"Evidence items:\", len(evidence_items))\n",
    "if evidence_items:\n",
    "    print(\"Sample OCR:\", evidence_items[0].source, evidence_items[0].ocr_text[:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fa100d",
   "metadata": {},
   "source": [
    "### 5.1 Optional captioning (bonus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed7e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CAPTIONING = False\n",
    "if USE_CAPTIONING:\n",
    "    from transformers import pipeline\n",
    "    captioner = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")\n",
    "    for ei in evidence_items:\n",
    "        cap = captioner(Image.open(ei.image_path).convert(\"RGB\"), max_new_tokens=40)[0][\"generated_text\"]\n",
    "        cap = re.sub(r\"\\s+\", \" \", cap).strip()\n",
    "        ei.caption_text = cap\n",
    "        ei.evidence_text = (ei.ocr_text + \"\\n\" + cap).strip()\n",
    "    print(\"Captioning complete.\")\n",
    "else:\n",
    "    print(\"Captioning skipped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf21da5a",
   "metadata": {},
   "source": [
    "## 6) Chunking (page-based vs fixed-size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb23d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SubChunk:\n",
    "    chunk_id: str\n",
    "    doc_id: str\n",
    "    page_num: int\n",
    "    text: str\n",
    "\n",
    "def fixed_size_chunk(text: str, words_per_chunk: int = 250, overlap: int = 40) -> List[str]:\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = min(len(words), start + words_per_chunk)\n",
    "        chunks.append(\" \".join(words[start:end]))\n",
    "        if end == len(words):\n",
    "            break\n",
    "        start = max(0, end - overlap)\n",
    "    return chunks\n",
    "\n",
    "sub_chunks = []\n",
    "for pc in page_chunks:\n",
    "    for j, t in enumerate(fixed_size_chunk(pc.text, 250, 40)):\n",
    "        sub_chunks.append(SubChunk(f\"{pc.doc_id}::p{pc.page_num}::c{j+1}\", pc.doc_id, pc.page_num, t))\n",
    "\n",
    "print(\"Page chunks:\", len(page_chunks))\n",
    "print(\"Fixed-size chunks:\", len(sub_chunks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0840be09",
   "metadata": {},
   "source": [
    "## 7) Indexing & retrieval (dense + sparse + rerank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b3a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> List[str]:\n",
    "    return [t.lower() for t in re.findall(r\"[a-zA-Z0-9]+\", text)]\n",
    "\n",
    "# --- Embeddings (dense retrieval) ---\n",
    "# If SentenceTransformers is available, we use it. Otherwise, we fall back to TF-IDF vectors.\n",
    "if USE_ST:\n",
    "    embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    def embed_texts(texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "        return embedder.encode(\n",
    "            texts, batch_size=batch_size, show_progress_bar=True,\n",
    "            convert_to_numpy=True, normalize_embeddings=True\n",
    "        )\n",
    "else:\n",
    "    # TF-IDF fallback (acts as a \"dense-ish\" baseline)\n",
    "    tfidf_vec = TfidfVectorizer(max_features=50000, ngram_range=(1,2))\n",
    "    _tfidf_fitted = False\n",
    "\n",
    "    def embed_texts(texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "        global _tfidf_fitted\n",
    "        X = tfidf_vec.fit_transform(texts) if not _tfidf_fitted else tfidf_vec.transform(texts)\n",
    "        _tfidf_fitted = True\n",
    "        X = normalize(X)\n",
    "        return X.toarray().astype(np.float32)\n",
    "\n",
    "def build_faiss_ip(vectors: np.ndarray):\n",
    "    dim = vectors.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(vectors.astype(np.float32))\n",
    "    return index\n",
    "\n",
    "TEXT_CORPUS_A = page_chunks\n",
    "TEXT_CORPUS_B = sub_chunks\n",
    "\n",
    "texts_A = [c.text for c in TEXT_CORPUS_A]\n",
    "vecs_A = embed_texts(texts_A) if texts_A else np.zeros((0,384), dtype=np.float32)\n",
    "faiss_A = build_faiss_ip(vecs_A) if len(texts_A)>0 else None\n",
    "bm25_A = BM25Okapi([tokenize(t) for t in texts_A]) if len(texts_A)>0 else None\n",
    "\n",
    "texts_B = [c.text for c in TEXT_CORPUS_B]\n",
    "vecs_B = embed_texts(texts_B) if texts_B else np.zeros((0,384), dtype=np.float32)\n",
    "faiss_B = build_faiss_ip(vecs_B) if len(texts_B)>0 else None\n",
    "bm25_B = BM25Okapi([tokenize(t) for t in texts_B]) if len(texts_B)>0 else None\n",
    "\n",
    "evid_texts = [e.evidence_text for e in evidence_items]\n",
    "evid_vecs = embed_texts(evid_texts) if evid_texts else np.zeros((0,384), dtype=np.float32)\n",
    "faiss_E = build_faiss_ip(evid_vecs) if len(evid_texts)>0 else None\n",
    "\n",
    "print(\"Indexes ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92192462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_search(query: str, index, corpus, top_k: int = 5):\n",
    "    if index is None or len(corpus)==0:\n",
    "        return []\n",
    "    qv = embed_texts([query])\n",
    "    scores, idxs = index.search(qv.astype(np.float32), top_k)\n",
    "    out = []\n",
    "    for s, i in zip(scores[0], idxs[0]):\n",
    "        if int(i) >= 0:\n",
    "            out.append((float(s), corpus[int(i)]))\n",
    "    return out\n",
    "\n",
    "def sparse_search(query: str, bm25, corpus, top_k: int = 5):\n",
    "    if bm25 is None or len(corpus)==0:\n",
    "        return []\n",
    "    scores = bm25.get_scores(tokenize(query))\n",
    "    top = np.argsort(scores)[::-1][:top_k]\n",
    "    return [(float(scores[i]), corpus[int(i)]) for i in top]\n",
    "\n",
    "def hybrid_fuse(dense_res, sparse_res, alpha: float = 0.5, top_k: int = 5):\n",
    "    def k(item): return getattr(item, \"chunk_id\", getattr(item, \"evid_id\", str(item)))\n",
    "    dense_rank = {k(it): r for r, (_, it) in enumerate(dense_res, start=1)}\n",
    "    sparse_rank = {k(it): r for r, (_, it) in enumerate(sparse_res, start=1)}\n",
    "    keys = set(dense_rank) | set(sparse_rank)\n",
    "    fused = []\n",
    "    for key in keys:\n",
    "        dr = dense_rank.get(key, len(dense_res)+1)\n",
    "        sr = sparse_rank.get(key, len(sparse_res)+1)\n",
    "        score = alpha*(1.0/dr) + (1-alpha)*(1.0/sr)\n",
    "        obj = next((it for _, it in dense_res if k(it)==key), None) or next((it for _, it in sparse_res if k(it)==key), None)\n",
    "        fused.append((score, obj))\n",
    "    fused.sort(key=lambda x: x[0], reverse=True)\n",
    "    return fused[:top_k]\n",
    "\n",
    "# --- Reranker (optional) ---\n",
    "reranker = None\n",
    "if USE_ST and USE_RERANK:\n",
    "    try:\n",
    "        reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "    except Exception as e:\n",
    "        reranker = None\n",
    "        USE_RERANK = False\n",
    "        print(\"⚠️ Reranker unavailable, continuing without reranking. Error:\", e)\n",
    "\n",
    "\n",
    "def rerank(query: str, items, get_text, top_k=5):\n",
    "    if reranker is None:\n",
    "        return list(items)[:top_k]\n",
    "\n",
    "    if not items:\n",
    "        return []\n",
    "    scores = reranker.predict([(query, get_text(it)) for it in items])\n",
    "    ranked = sorted(zip(scores, items), key=lambda x: x[0], reverse=True)\n",
    "    return [it for _, it in ranked[:top_k]]\n",
    "\n",
    "def retrieve_text(query: str, chunking: str = \"page\", method: str = \"hybrid\", top_k: int = 5, alpha: float = 0.5, use_rerank: bool = True):\n",
    "    if chunking == \"page\":\n",
    "        corpus, index, bm25 = TEXT_CORPUS_A, faiss_A, bm25_A\n",
    "    else:\n",
    "        corpus, index, bm25 = TEXT_CORPUS_B, faiss_B, bm25_B\n",
    "\n",
    "    if method == \"dense\":\n",
    "        res = dense_search(query, index, corpus, top_k=max(10, top_k))\n",
    "        items = [it for _, it in res]\n",
    "    elif method == \"sparse\":\n",
    "        res = sparse_search(query, bm25, corpus, top_k=max(10, top_k))\n",
    "        items = [it for _, it in res]\n",
    "    else:\n",
    "        d = dense_search(query, index, corpus, top_k=max(10, top_k))\n",
    "        s = sparse_search(query, bm25, corpus, top_k=max(10, top_k))\n",
    "        res = hybrid_fuse(d, s, alpha=alpha, top_k=max(10, top_k))\n",
    "        items = [it for _, it in res]\n",
    "\n",
    "    if use_rerank:\n",
    "        return rerank(query, items, lambda it: it.text, top_k=top_k)\n",
    "    return items[:top_k]\n",
    "\n",
    "def retrieve_evidence(query: str, top_k: int = 3):\n",
    "    res = dense_search(query, faiss_E, evidence_items, top_k=top_k)\n",
    "    return [it for _, it in res]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bbdcd5",
   "metadata": {},
   "source": [
    "## 8) Evidence pack + citations (product output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1803c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cite_text(it): return f\"[{it.doc_id} p{it.page_num}]\"\n",
    "def cite_fig(ei): return f\"[{os.path.splitext(ei.source)[0]}]\"\n",
    "\n",
    "def build_evidence_pack(question: str, chunking=\"page\", method=\"hybrid\", top_k_text=4, top_k_fig=2):\n",
    "    txt = retrieve_text(question, chunking=chunking, method=method, top_k=top_k_text, use_rerank=True)\n",
    "    figs = retrieve_evidence(question, top_k=top_k_fig)\n",
    "    pack = []\n",
    "    for it in txt:\n",
    "        pack.append({\"type\":\"text\", \"cite\": cite_text(it), \"content\": it.text[:800]})\n",
    "    for ei in figs:\n",
    "        pack.append({\"type\":\"figure\", \"cite\": cite_fig(ei), \"content\": (ei.evidence_text or \"\")[:800], \"path\": ei.image_path})\n",
    "    return pack\n",
    "\n",
    "ep = build_evidence_pack(QUERIES[0][\"question\"])\n",
    "for e in ep:\n",
    "    print(e[\"cite\"], e[\"type\"], e[\"content\"][:120])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b697d",
   "metadata": {},
   "source": [
    "## 9) Grounded response (LLM/VLM) — connect Gemini/HF if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b30735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_prompt(question: str, evidence_pack: list) -> str:\n",
    "    evidence_lines = [f'{e[\"cite\"]} {e[\"content\"]}' for e in evidence_pack]\n",
    "    evidence_block = \"\\n\\n\".join(evidence_lines)\n",
    "    return f\"\"\"You are a grounded assistant. Use ONLY the evidence below.\n",
    "Every key claim must cite evidence like [doc p#] or [fig1].\n",
    "If the evidence is insufficient, respond exactly:\n",
    "Not enough evidence in the retrieved context.\n",
    "\n",
    "Evidence:\n",
    "{evidence_block}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer (with citations):\n",
    "\"\"\"\n",
    "\n",
    "def generate_answer(prompt: str, image_paths: Optional[list]=None) -> str:\n",
    "    # TODO: connect Gemini or a HF generator\n",
    "    return \"TODO: connect Gemini/HF LLM. For demo: summarize evidence with citations.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677444a1",
   "metadata": {},
   "source": [
    "## 10) Demo loop (stakeholder-facing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6db4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_one(question: str, chunking=\"page\", method=\"hybrid\"):\n",
    "    ep = build_evidence_pack(question, chunking=chunking, method=method)\n",
    "    prompt = rag_prompt(question, ep)\n",
    "    ans = generate_answer(prompt, image_paths=[e.get(\"path\") for e in ep if e.get(\"type\")==\"figure\"])\n",
    "    return ep, ans\n",
    "\n",
    "for q in QUERIES:\n",
    "    ep, ans = demo_one(q[\"question\"])\n",
    "    print(\"\\n=== \", q[\"id\"], \" ===\")\n",
    "    print(\"Q:\", q[\"question\"])\n",
    "    print(\"Top evidence citations:\", [e[\"cite\"] for e in ep])\n",
    "    print(\"Answer:\", ans[:400])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af324fc",
   "metadata": {},
   "source": [
    "## 11) Week 3 acceptance tests (CS 5588)\n",
    "Fill in after running your demo:\n",
    "- Does the evidence pack include the must-cite items for Q1/Q2?\n",
    "- Does Q3 properly refuse with “Not enough evidence…”?\n",
    "- Is the output understandable to your target user?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b64e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCEPTANCE_CHECKLIST = [\n",
    "    {\"qid\":\"Q1\", \"must_cite_expected\":\"TODO\", \"pass_fail\":\"TODO\", \"notes\":\"TODO\"},\n",
    "    {\"qid\":\"Q2\", \"must_cite_expected\":\"TODO\", \"pass_fail\":\"TODO\", \"notes\":\"TODO\"},\n",
    "    {\"qid\":\"Q3\", \"must_cite_expected\":\"(none) — should refuse\", \"pass_fail\":\"TODO\", \"notes\":\"TODO\"},\n",
    "]\n",
    "ACCEPTANCE_CHECKLIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55742c0",
   "metadata": {},
   "source": [
    "## 11.5 Team work items (project enhancement)\n",
    "\n",
    "Use this hands-on to **advance your semester project**. Each team member should “own” at least one deliverable below.\n",
    "\n",
    "**Product Lead (Applicability)**\n",
    "- Update your project **persona + workflow** so the multimodal RAG module is a *core feature*, not an add-on.\n",
    "- Write 3 stakeholder tasks that map to your product’s real decision points (2 require text+figure evidence, 1 must refuse).\n",
    "\n",
    "**Systems Lead (Integration)**\n",
    "- Replace the toy dataset with your **project-domain PDFs + figures**.\n",
    "- Add **metadata fields** that matter to your domain (e.g., policy date, version, department, study cohort, device model).\n",
    "- Implement a clean **`retrieve()` API** your final demo can reuse.\n",
    "\n",
    "**Evaluation & Risk Lead (Shipping readiness)**\n",
    "- Build a tiny evaluation table: *Task × Method × P@5 × R@10 × Faithfulness*.\n",
    "- Add one real failure scenario + mitigation UX (warnings, “show evidence” first, or human-in-the-loop flag).\n",
    "- Draft the “If we shipped this” plan: data refresh, monitoring, and governance rule.\n",
    "\n",
    "**Bonus (Optional)**\n",
    "- Add a minimal UI (Gradio/Streamlit) that shows: question → evidence pack → answer with citations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb6e514",
   "metadata": {},
   "source": [
    "## 12) Week 3 deliverables (CS 5588)\n",
    "- Product Brief completed (persona, problem, value, success metrics)\n",
    "- Demo run for Q1–Q3 with citations (screenshots encouraged)\n",
    "- 1 failure case + mitigation plan (risk + fix)\n",
    "- Repo link submitted in the survey\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CS5588_Week3_HandsOn_Multimodal_RAG_Product.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
