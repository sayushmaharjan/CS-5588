{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2a10fbb4",
      "metadata": {},
      "source": [
        "# CS 5588 \u2014 Data Science Capstone\n",
        "## Week 4 Hands-On \u2014 Capstone Product Integration Sprint  \n",
        "**Deadline:** Feb. 12, 2026 (Thu), Midnight\n",
        "\n",
        "### What this week is about\n",
        "This Week 4 hands-on upgrades your Week 3 prototype into a **capstone-ready product module**:\n",
        "- **Application integration** (a simple UI or endpoint that demonstrates a real workflow)\n",
        "- **Operational logging + monitoring** (so you can measure usage and failures)\n",
        "- **Impact-focused evaluation** (not only IR metrics \u2014 also stakeholder/product impact)\n",
        "- **Deployment readiness plan** (architecture + run instructions)\n",
        "- **Failure & risk analysis** (what can go wrong, how you detect/mitigate)\n",
        "\n",
        "### Submission policy\n",
        "- **Team deliverables (GitHub):** code + notebook + brief/report + screenshots/diagram  \n",
        "- **Individual reflection (Canvas/Survey):** one short paragraph\n",
        "\n",
        "> This notebook is a **template**. Replace placeholders with your project specifics (data, users, goals, models).\n",
        "\n",
        "---\n",
        "\n",
        "## Recommended repo structure\n",
        "```\n",
        "/app/                 # Streamlit (or other) UI (recommended)\n",
        "/src/                 # reusable pipeline code (data + modeling + retrieval)\n",
        "/logs/                # monitoring logs (auto-created)\n",
        "/reports/             # integration brief + diagrams\n",
        "/notebooks/           # this notebook\n",
        "requirements.txt\n",
        "README.md\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Checklist (quick)\n",
        "- [ ] Week-4 Integration Brief completed (Section 2)\n",
        "- [ ] Working app demo (Section 6)\n",
        "- [ ] Logging file created and populated (Section 4\u20135)\n",
        "- [ ] Impact evaluation + technical metrics (Section 5)\n",
        "- [ ] Deployment plan + architecture diagram (Section 7)\n",
        "- [ ] One realistic failure/risk + mitigation (Section 8)\n",
        "- [ ] Individual reflection (Section 9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf50b58b",
      "metadata": {},
      "source": [
        "## 1) Team & project metadata (Required)\n",
        "Fill these fields first. They will be reused in your report and README."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u2705 Step 0.5 \u2014 Fill in your project info (required)\n",
        "\n",
        "Before generating files, update the configuration values below. This prevents generic submissions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# CS5588_WEEK4_CONFIG_VALIDATION\n",
        "import os\n",
        "\n",
        "# ---- REQUIRED: edit these ----\n",
        "TEAM_NAME = \"<REPLACE_WITH_TEAM_NAME>\"\n",
        "PROJECT_TITLE = \"<REPLACE_WITH_PROJECT_TITLE>\"\n",
        "TARGET_USER = \"<REPLACE_WITH_TARGET_USER_OR_STAKEHOLDER>\"\n",
        "\n",
        "# Optional\n",
        "DEPLOYMENT_TARGET = \"Streamlit Cloud\"  # or HuggingFace Spaces, Render, etc.\n",
        "\n",
        "def _require_filled(label, value):\n",
        "    if value.strip().startswith('<REPLACE_') or value.strip() == \"\":\n",
        "        raise ValueError(f\"Please edit {label} at the top of this cell before continuing.\")\n",
        "\n",
        "_require_filled('TEAM_NAME', TEAM_NAME)\n",
        "_require_filled('PROJECT_TITLE', PROJECT_TITLE)\n",
        "_require_filled('TARGET_USER', TARGET_USER)\n",
        "\n",
        "print('\u2705 Config looks good')\n",
        "print('TEAM_NAME:', TEAM_NAME)\n",
        "print('PROJECT_TITLE:', PROJECT_TITLE)\n",
        "print('TARGET_USER:', TARGET_USER)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b213d49c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json, time\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "@dataclass\n",
        "class Week4Config:\n",
        "    course: str = \"CS 5588 Data Science Capstone\"\n",
        "    week: str = \"Week 4\"\n",
        "    deadline: str = \"2026-02-12 23:59\"\n",
        "    team_name: str = \"TEAM_NAME\"\n",
        "    project_title: str = \"PROJECT_TITLE\"\n",
        "    stakeholder: str = \"TARGET_USER / STAKEHOLDER\"\n",
        "    problem_statement: str = \"1\u20132 sentences: what real problem are you solving?\"\n",
        "    data_summary: str = \"What data (sources/modalities) does your project use?\"\n",
        "    model_summary: str = \"What model(s) do you use (baseline + main)?\"\n",
        "    app_dir: str = \"./app\"\n",
        "    src_dir: str = \"./src\"\n",
        "    logs_dir: str = \"./logs\"\n",
        "    reports_dir: str = \"./reports\"\n",
        "    log_file: str = \"./logs/week4_events.csv\"\n",
        "\n",
        "cfg = Week4Config()\n",
        "Path(cfg.app_dir).mkdir(parents=True, exist_ok=True)\n",
        "Path(cfg.src_dir).mkdir(parents=True, exist_ok=True)\n",
        "Path(cfg.logs_dir).mkdir(parents=True, exist_ok=True)\n",
        "Path(cfg.reports_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "cfg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0868900",
      "metadata": {},
      "source": [
        "## 2) Week-4 Capstone Integration Brief (Required)\n",
        "Create a **1-page** brief (can be in `reports/week4_integration_brief.md` or a README section).\n",
        "\n",
        "Include:\n",
        "1. **Where this module fits** in your capstone architecture  \n",
        "2. **Primary user workflow** (what the user does end-to-end)  \n",
        "3. **Success metrics** (product/impact metrics + technical metrics)  \n",
        "4. **Risks if it fails** (stakeholder harm / wrong decision / wasted time)  \n",
        "5. **Next sprint** (what you would build next)\n",
        "\n",
        "Below is a starter you can export to Markdown.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3de8750",
      "metadata": {},
      "outputs": [],
      "source": [
        "brief_path = Path(cfg.reports_dir) / \"week4_integration_brief.md\"\n",
        "\n",
        "brief_template = f\"\"\"# Week 4 Integration Brief \u2014 {cfg.project_title}\n",
        "**Team:** {cfg.team_name}  \n",
        "**Stakeholder/User:** {cfg.stakeholder}  \n",
        "**Problem:** {cfg.problem_statement}\n",
        "\n",
        "## 1) Module placement in capstone system\n",
        "- Upstream inputs:\n",
        "- Module responsibilities:\n",
        "- Downstream outputs:\n",
        "\n",
        "## 2) User workflow (end-to-end)\n",
        "1.\n",
        "2.\n",
        "3.\n",
        "\n",
        "## 3) Success metrics\n",
        "### Product / impact metrics (required)\n",
        "- Time-to-decision:\n",
        "- Trust/verification signals:\n",
        "- Adoption/usage signal:\n",
        "\n",
        "### Technical metrics (recommended)\n",
        "- Quality (e.g., accuracy/F1, Precision@K/Recall@K, calibration, etc.):\n",
        "- Latency:\n",
        "- Failure rate:\n",
        "\n",
        "## 4) Failure & risk (what happens if wrong?)\n",
        "- Likely failure:\n",
        "- Impact:\n",
        "- Mitigation:\n",
        "\n",
        "## 5) Next sprint plan\n",
        "- Next feature:\n",
        "- Data improvement:\n",
        "- Evaluation improvement:\n",
        "\"\"\"\n",
        "\n",
        "# Write (or overwrite) template file\n",
        "brief_path.write_text(brief_template, encoding=\"utf-8\")\n",
        "print(\"Wrote:\", brief_path)\n",
        "print(\"\\nPreview (first 25 lines):\\n\")\n",
        "print(\"\\n\".join(brief_template.splitlines()[:25]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41f8e298",
      "metadata": {},
      "source": [
        "## 3) Data + modeling hook (Project-aligned)\n",
        "Week 4 must use **your capstone project data and models**.\n",
        "\n",
        "- If you are building a **RAG / search / recommender**: wire your retrieval + generation here.\n",
        "- If you are building a **predictive model**: wire your training/inference function here.\n",
        "- If you are building a **dashboard / analytics product**: wire your data processing + visualization logic here.\n",
        "\n",
        "Below is a **minimal runnable stub** so this notebook executes even without your data.\n",
        "Replace the stubs with your actual project code from Week 3.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3a8a11b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Replace these with your real project pipeline imports ---\n",
        "# from src.pipeline import load_data, run_inference, retrieve_evidence, generate_output\n",
        "\n",
        "def load_data_stub():\n",
        "    # Example synthetic dataset (replace with your real dataset loader)\n",
        "    df = pd.DataFrame({\n",
        "        \"id\": [1, 2, 3],\n",
        "        \"text\": [\n",
        "            \"Project doc: model deployment requires monitoring.\",\n",
        "            \"Project doc: user trust improves with evidence and citations.\",\n",
        "            \"Project doc: define success metrics and failure cases.\"\n",
        "        ],\n",
        "        \"label\": [1, 1, 0]\n",
        "    })\n",
        "    return df\n",
        "\n",
        "def predict_stub(df: pd.DataFrame, query: str):\n",
        "    # Replace with your real model inference\n",
        "    # For demo: return top rows containing keyword overlap\n",
        "    scores = df[\"text\"].str.lower().apply(lambda t: sum(w in t for w in query.lower().split()))\n",
        "    top = df.assign(score=scores).sort_values(\"score\", ascending=False).head(3)\n",
        "    return top\n",
        "\n",
        "df_demo = load_data_stub()\n",
        "df_demo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "657b873d",
      "metadata": {},
      "source": [
        "## 4) Monitoring & logging (Required)\n",
        "You must implement **automatic logging** for every user interaction / query.\n",
        "\n",
        "Minimum columns (recommended):\n",
        "- timestamp\n",
        "- event_type (query / feedback / error)\n",
        "- user_task_type (what workflow this supports)\n",
        "- config (model/retrieval settings)\n",
        "- latency_ms\n",
        "- output_quality_signal (e.g., faithfulness pass/fail, confidence, error flag)\n",
        "- artifact_ids (evidence ids, record ids, etc.)\n",
        "\n",
        "This creates the foundation for **production-style monitoring** and **capstone evaluation**.\n",
        "\n",
        "\n",
        "**Mapping to the Week 4 handout terminology (1-to-1):**\n",
        "- `artifact_ids` \u2192 **evidence IDs**\n",
        "- `model_or_mode` \u2192 **retrieval configuration**\n",
        "- `quality_signal` \u2192 **confidence/faithfulness indicator**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea76d1e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "LOG_COLUMNS = [\n",
        "    \"timestamp\",\n",
        "    \"event_type\",\n",
        "    \"user_task_type\",\n",
        "    \"model_or_mode\",\n",
        "    \"latency_ms\",\n",
        "    \"artifact_ids\",\n",
        "    \"quality_signal\",\n",
        "    \"notes\"\n",
        "]\n",
        "\n",
        "def ensure_csv(path: str, header: list):\n",
        "    p = Path(path)\n",
        "    p.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if not p.exists():\n",
        "        with open(p, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(header)\n",
        "\n",
        "ensure_csv(cfg.log_file, LOG_COLUMNS)\n",
        "print(\"Logging to:\", cfg.log_file)\n",
        "\n",
        "def log_event(event_type: str,\n",
        "              user_task_type: str,\n",
        "              model_or_mode: str,\n",
        "              latency_ms: float,\n",
        "              artifact_ids,\n",
        "              quality_signal: str,\n",
        "              notes: str = \"\"):\n",
        "    row = [\n",
        "        datetime.now(timezone.utc).isoformat(),\n",
        "        event_type,\n",
        "        user_task_type,\n",
        "        model_or_mode,\n",
        "        round(float(latency_ms), 2),\n",
        "        json.dumps(artifact_ids, ensure_ascii=False),\n",
        "        quality_signal,\n",
        "        notes\n",
        "    ]\n",
        "    with open(cfg.log_file, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.writer(f).writerow(row)\n",
        "\n",
        "# Demo log entry\n",
        "log_event(\n",
        "    event_type=\"query\",\n",
        "    user_task_type=\"demo\",\n",
        "    model_or_mode=\"stub\",\n",
        "    latency_ms=12.3,\n",
        "    artifact_ids=[1,2],\n",
        "    quality_signal=\"OK\",\n",
        "    notes=\"demo event\"\n",
        ")\n",
        "\n",
        "pd.read_csv(cfg.log_file).tail(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6a30529",
      "metadata": {},
      "source": [
        "## 5) Week-4 evaluation: Impact + Technical metrics (Required)\n",
        "Capstone evaluation must include **impact-oriented metrics**, not only technical ones.\n",
        "\n",
        "### A) Impact metrics (required)\n",
        "Choose 2\u20133 that match your stakeholder workflow:\n",
        "- time-to-decision (before vs after)\n",
        "- trust/verification rate (e.g., citations shown, evidence opened)\n",
        "- task success rate (user can complete task)\n",
        "- adoption signal (weekly active usage in demo, number of queries run)\n",
        "\n",
        "### B) Technical metrics (recommended)\n",
        "Pick metrics that match your system type:\n",
        "- Classification/regression: accuracy/F1/AUC/MAE + calibration\n",
        "- Retrieval/RAG: Precision@K/Recall@K + citation coverage + refusal correctness\n",
        "- Forecasting: MAE/MAPE/CRPS, interval coverage, etc.\n",
        "\n",
        "Below is a small example that computes simple metrics from your demo data.\n",
        "Replace with your project-specific metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81a05317",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example technical metrics (replace for your system)\n",
        "# Here we pretend \"label\" is the target and \"score>0\" is predicted positive.\n",
        "\n",
        "def compute_demo_metrics(df: pd.DataFrame):\n",
        "    y_true = df[\"label\"].values\n",
        "    y_pred = (df[\"text\"].str.contains(\"trust|monitoring|metrics\", case=False, regex=True)).astype(int).values\n",
        "    acc = (y_true == y_pred).mean()\n",
        "    return {\"accuracy_demo\": float(acc)}\n",
        "\n",
        "metrics = compute_demo_metrics(df_demo)\n",
        "metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb913dc1",
      "metadata": {},
      "source": [
        "## 6) Build your capstone demo app (Required)\n",
        "Your team must expose the module via an application interface:\n",
        "- Streamlit UI (recommended), OR\n",
        "- an API endpoint + simple client, OR\n",
        "- a dashboard component integrated into your project\n",
        "\n",
        "### Required app behavior\n",
        "- Accept user input (question / task / parameters)\n",
        "- Produce output aligned to your project workflow\n",
        "- Display artifacts (evidence / records / plots) as appropriate\n",
        "- Log events automatically to `logs/week4_events.csv`\n",
        "\n",
        "Below is a Streamlit skeleton generator that you can commit to `/app/main.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc315c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "streamlit_app = r'''\n",
        "import json, time\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "\n",
        "# --- Customize: import your project pipeline here ---\n",
        "# from src.pipeline import load_data, run_inference\n",
        "\n",
        "LOG_FILE = \"logs/week4_events.csv\"\n",
        "LOG_COLUMNS = [\"timestamp\",\"event_type\",\"user_task_type\",\"model_or_mode\",\"latency_ms\",\"artifact_ids\",\"quality_signal\",\"notes\"]\n",
        "\n",
        "def ensure_csv(path: str, header):\n",
        "    p = Path(path)\n",
        "    p.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if not p.exists():\n",
        "        pd.DataFrame(columns=header).to_csv(p, index=False)\n",
        "\n",
        "def log_event(event_type, user_task_type, model_or_mode, latency_ms, artifact_ids, quality_signal, notes=\"\"):\n",
        "    ensure_csv(LOG_FILE, LOG_COLUMNS)\n",
        "    df = pd.read_csv(LOG_FILE)\n",
        "    row = {\n",
        "        \"timestamp\": pd.Timestamp.utcnow().isoformat(),\n",
        "        \"event_type\": event_type,\n",
        "        \"user_task_type\": user_task_type,\n",
        "        \"model_or_mode\": model_or_mode,\n",
        "        \"latency_ms\": round(float(latency_ms), 2),\n",
        "        \"artifact_ids\": json.dumps(artifact_ids, ensure_ascii=False),\n",
        "        \"quality_signal\": quality_signal,\n",
        "        \"notes\": notes\n",
        "    }\n",
        "    df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
        "    df.to_csv(LOG_FILE, index=False)\n",
        "\n",
        "# --- Demo stub (replace) ---\n",
        "def load_data_stub():\n",
        "    return pd.DataFrame({\n",
        "        \"id\":[1,2,3],\n",
        "        \"text\":[\n",
        "            \"Project doc: model deployment requires monitoring.\",\n",
        "            \"Project doc: user trust improves with evidence and citations.\",\n",
        "            \"Project doc: define success metrics and failure cases.\"\n",
        "        ]\n",
        "    })\n",
        "\n",
        "def run_inference_stub(df, query: str):\n",
        "    scores = df[\"text\"].str.lower().apply(lambda t: sum(w in t for w in query.lower().split()))\n",
        "    top = df.assign(score=scores).sort_values(\"score\", ascending=False).head(3)\n",
        "    artifacts = top[\"id\"].tolist()\n",
        "    output = top[[\"id\",\"text\",\"score\"]].to_dict(orient=\"records\")\n",
        "    return output, artifacts\n",
        "\n",
        "# --- UI ---\n",
        "st.set_page_config(page_title=\"CS5588 Week 4 \u2014 Capstone Demo\", layout=\"wide\")\n",
        "st.title(\"CS 5588 Week 4 \u2014 Capstone Product Integration Demo\")\n",
        "st.caption(\"Project-aligned module + monitoring logs + deployment readiness\")\n",
        "\n",
        "df = load_data_stub()\n",
        "\n",
        "user_task_type = st.selectbox(\"User task type (workflow)\", [\"analysis\", \"search\", \"decision-support\", \"reporting\", \"other\"])\n",
        "model_or_mode = st.selectbox(\"Model/mode\", [\"baseline\", \"main\", \"ablation\"])\n",
        "\n",
        "query = st.text_area(\"Enter your question / task input\", height=120)\n",
        "run_btn = st.button(\"Run\")\n",
        "\n",
        "if run_btn and query.strip():\n",
        "    t0 = time.time()\n",
        "    output, artifacts = run_inference_stub(df, query)\n",
        "    latency_ms = (time.time() - t0) * 1000\n",
        "\n",
        "    # --- Response panel ---\n",
        "    st.subheader(\"Response\")\n",
        "    st.json(output)\n",
        "\n",
        "    # --- Evidence panel (explicit, required by handout) ---\n",
        "    # In your real system, this should render retrieved chunks/records with IDs + short text/snippet + source link/citation.\n",
        "    with st.expander(\"Evidence (IDs + preview)\", expanded=True):\n",
        "        st.write(\"Evidence IDs:\", artifacts)\n",
        "        # Stub evidence table (replace with real evidence objects)\n",
        "        evidence_rows = [{\"evidence_id\": a, \"preview\": \"Replace with evidence snippet/source\"} for a in artifacts]\n",
        "        st.table(pd.DataFrame(evidence_rows))\n",
        "\n",
        "    # --- Metrics panel (explicit, required by handout) ---\n",
        "    st.subheader(\"Metrics\")\n",
        "    st.write({\n",
        "        \"latency_ms\": round(latency_ms, 2),\n",
        "        \"retrieval_configuration\": model_or_mode,   # maps to handout: retrieval configuration\n",
        "        \"confidence_or_faithfulness\": \"OK\" if len(artifacts) > 0 else \"LOW\"  # maps to handout: confidence/faithfulness indicator\n",
        "    })\n",
        "\n",
        "    # Simple quality signal stub\n",
        "    quality_signal = \"OK\" if len(artifacts) > 0 else \"LOW\"\n",
        "\n",
        "    log_event(\n",
        "        event_type=\"query\",\n",
        "        user_task_type=user_task_type,\n",
        "        model_or_mode=model_or_mode,\n",
        "        latency_ms=latency_ms,\n",
        "        artifact_ids=artifacts,\n",
        "        quality_signal=quality_signal,\n",
        "        notes=\"week4 demo\"\n",
        "    )\n",
        "    st.success(f\"Logged event to {LOG_FILE}\")\n",
        "'''\n",
        "\n",
        "app_path = Path(cfg.app_dir) / \"main.py\"\n",
        "app_path.write_text(streamlit_app, encoding=\"utf-8\")\n",
        "print(\"Wrote Streamlit app:\", app_path)\n",
        "print(\"\\nRun locally (terminal):\")\n",
        "print(\"  pip install -r requirements.txt\")\n",
        "print(\"  streamlit run app/main.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1) Generate `requirements.txt` and a starter `README.md` (Recommended)\n",
        "\n",
        "For deployment reproducibility, create a minimal dependency list and run instructions.\n",
        "If these files already exist, this cell will **not overwrite** them unless `FORCE_OVERWRITE=True`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "FORCE_OVERWRITE = False\n",
        "\n",
        "req_path = Path('requirements.txt')\n",
        "readme_path = Path('README.md')\n",
        "\n",
        "requirements_text = \"\"\"streamlit\n",
        "pandas\n",
        "numpy\n",
        "requests\n",
        "python-dotenv\n",
        "# Add your ML/LLM packages below (examples):\n",
        "# scikit-learn\n",
        "# torch\n",
        "# transformers\n",
        "\"\"\"\n",
        "\n",
        "readme_text = f\"\"\"# {cfg.project_title or 'CS 5588 Capstone'} \u2014 Week 4 Hands-On\n",
        "\n",
        "**Team:** {cfg.team_name}  \n",
        "**Stakeholder/User:** {cfg.stakeholder}\n",
        "\n",
        "## Deployment Link\n",
        "- (Paste your deployed URL here)\n",
        "\n",
        "## Run Locally\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "streamlit run app/main.py\n",
        "```\n",
        "\n",
        "## What this demo does\n",
        "- Briefly describe the product workflow (2\u20134 bullets).\n",
        "\n",
        "## Logs\n",
        "- Logs are written to `logs/week4_events.csv`.\n",
        "\n",
        "## Week 4 Metrics Summary\n",
        "- Impact metrics:\n",
        "- Technical metrics:\n",
        "\n",
        "## Failure & Risk\n",
        "- Link or summary of `reports/week4_failure_risk.md`.\n",
        "\"\"\"\n",
        "\n",
        "def write_if_missing(path: Path, content: str):\n",
        "    if path.exists() and not FORCE_OVERWRITE:\n",
        "        print(f\"Exists (skipping): {path}\")\n",
        "        return\n",
        "    path.write_text(content.strip() + \"\\n\", encoding='utf-8')\n",
        "    print(f\"Wrote: {path}\")\n",
        "\n",
        "write_if_missing(req_path, requirements_text)\n",
        "write_if_missing(readme_path, readme_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f1e2d99",
      "metadata": {},
      "source": [
        "## 7) Deployment readiness plan (Required)\n",
        "In your README or `reports/`, include:\n",
        "- **Deployment target:** (HF Spaces / Streamlit Cloud / Render / Railway)\n",
        "- **Data handling:** what is included vs excluded from repo\n",
        "- **Monitoring plan:** what you log and how you review it\n",
        "- **Governance / guardrails:** what the system refuses to do, and why\n",
        "- **Architecture diagram:** a simple block diagram of components and data flow\n",
        "\n",
        "Below is a starter diagram description you can paste into your report (replace with your architecture).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bae87f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "arch_path = Path(cfg.reports_dir) / \"week4_architecture_notes.md\"\n",
        "arch_notes = f\"\"\"# Week 4 Deployment Readiness \u2014 Architecture Notes\n",
        "\n",
        "## Deployment target\n",
        "- Hosting option:\n",
        "- Public link:\n",
        "\n",
        "## Components\n",
        "- UI: Streamlit (`/app/main.py`)\n",
        "- Core module: `/src/` (data + model + retrieval)\n",
        "- Logs: `/logs/week4_events.csv`\n",
        "- Data: `/data/` (not committed if private/large)\n",
        "\n",
        "## Data flow\n",
        "User \u2192 UI \u2192 Core module \u2192 Output + Artifacts \u2192 UI\n",
        "                         \u2198 log_event() \u2192 logs/week4_events.csv\n",
        "\n",
        "## Monitoring & governance\n",
        "- What gets logged:\n",
        "- Refusal / safe behavior:\n",
        "- What triggers an alert (failure signal):\n",
        "\n",
        "## Scaling considerations (short)\n",
        "- What becomes slow at 10\u00d7 data?\n",
        "- What caching/indexing would you add?\n",
        "\"\"\"\n",
        "arch_path.write_text(arch_notes, encoding=\"utf-8\")\n",
        "print(\"Wrote:\", arch_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2850390",
      "metadata": {},
      "source": [
        "## 8) Failure & risk analysis (Required)\n",
        "Document **one realistic deployment-level failure** and how you will detect/mitigate it.\n",
        "\n",
        "Examples:\n",
        "- Wrong evidence leads to wrong user decision\n",
        "- Data drift reduces model performance\n",
        "- Retrieval returns irrelevant context causing hallucination\n",
        "- Latency spikes make product unusable\n",
        "\n",
        "Below is a short template you can paste into your report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6995b535",
      "metadata": {},
      "outputs": [],
      "source": [
        "risk_path = Path(cfg.reports_dir) / \"week4_failure_risk.md\"\n",
        "risk_template = f\"\"\"# Week 4 Failure & Risk Analysis\n",
        "\n",
        "## Failure scenario (realistic)\n",
        "- What fails?\n",
        "- Example user input that triggers it:\n",
        "\n",
        "## Impact (stakeholder/product)\n",
        "- What wrong decision could be made?\n",
        "- What harm/cost could occur?\n",
        "\n",
        "## Detection signals (monitoring)\n",
        "- Which log fields reveal this?\n",
        "- Thresholds (example):\n",
        "\n",
        "## Mitigation\n",
        "- Guardrails/refusal conditions:\n",
        "- Model/data fix:\n",
        "- Evaluation fix:\n",
        "\n",
        "## Post-mortem plan\n",
        "- What would you change next sprint?\n",
        "\"\"\"\n",
        "risk_path.write_text(risk_template, encoding=\"utf-8\")\n",
        "print(\"Wrote:\", risk_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90b185f0",
      "metadata": {},
      "source": [
        "## 9) Individual reflection (Required, individual submission)\n",
        "Each student submits **one paragraph** addressing:\n",
        "\n",
        "1. What part of the capstone module is closest to production-ready?\n",
        "2. What is the biggest risk to deploying this?\n",
        "3. What would you build next sprint?\n",
        "\n",
        "Paste your paragraph in the individual survey/Canvas submission.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49f66662",
      "metadata": {},
      "source": [
        "## 10) Final deliverables (Team)\n",
        "Your GitHub repo should include:\n",
        "- `/app/main.py` (or equivalent app interface)\n",
        "- `/src/` (pipeline modules; reuse Week 3 work)\n",
        "- `/logs/week4_events.csv` (auto-created, with sample rows)\n",
        "- `/reports/week4_integration_brief.md`\n",
        "- `/reports/week4_architecture_notes.md`\n",
        "- `/reports/week4_failure_risk.md`\n",
        "- `requirements.txt`\n",
        "- `README.md` with:\n",
        "  - deployment link\n",
        "  - run instructions\n",
        "  - screenshots\n",
        "  - metrics summary (impact + technical)\n",
        "\n",
        "---\n",
        "\n",
        "### Tip (grading-friendly)\n",
        "Make sure a TA can:\n",
        "1) run `pip install -r requirements.txt`  \n",
        "2) run `streamlit run app/main.py`  \n",
        "3) see logs populate in `logs/week4_events.csv`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2e71684",
      "metadata": {},
      "source": [
        "## GitHub Deployment (Required Example)\n",
        "\n",
        "### Step 1 \u2014 Push your repository\n",
        "```bash\n",
        "git init\n",
        "git add .\n",
        "git commit -m \"Week4 capstone app\"\n",
        "git branch -M main\n",
        "git remote add origin https://github.com/<username>/<repo>.git\n",
        "git push -u origin main\n",
        "```\n",
        "\n",
        "### Step 2 \u2014 Deploy using GitHub-connected hosting\n",
        "Example: Streamlit Community Cloud\n",
        "\n",
        "1. Go to https://share.streamlit.io\n",
        "2. Click **New App**\n",
        "3. Select your GitHub repository\n",
        "4. Branch: `main`\n",
        "5. App path: `app/main.py`\n",
        "6. Click **Deploy**\n",
        "\n",
        "### Step 3 \u2014 Add deployment link to README\n",
        "Include the deployed URL in your repository README.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}